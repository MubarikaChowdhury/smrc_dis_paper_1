---
title: "Comparison of Confidence Interval in the OLS and Ridge Linear Regression Model: A Comparative study via Simulation"
format: #docx
    tandf-pdf:
      keep-tex: true  
    tandf-html: default
author:
  - name: Sultana Mubarika Rahman Chowdhury
    affiliations:
      - ref: biostat
    orcid: 
    email: schow034@fiu.edu
  - name: B.M. Golam Kibria
    affiliations:
      - ref: stat
  - name: Zoran Bursac
    affiliations:
      - ref: biostat
affiliations:
  - id: biostat
    name: Florida International University
    department: Biosatistics Department
    address: 11200 SW 8th St
    city: Miami, Fl
    country: USA
    postal-code: 33199
  - id: stat
    name: Florida International University
    department: Department of Mathematics and Statistics
    address: 11200 SW 8th St 
    city: Miami, Fl
    country: USA
    postal-code: 33199
abstract: |
  Write down the abstract here
keywords: 
  - Multiple Linear Regression
  - Ridge Regression
  - Multicolinearity
bibliography: bibliography.bib  
---

# Introduction

Multiple linear regression maps the relationship between two or more predictors and dependent variable to a linear equation. The aim is to predict the response variable using the independent variables. If X is a n×p full rank matrix of predictors and Y is a n × 1 vector of response variables the multiple linear regression can be explained as, $$Y = X \beta + \epsilon,  $$ {#eq-test} where, $\beta$ is an p × 1 unknown regression paramaters and $\epsilon$ is the n × 1 vector of error with mean zero and equal variance.Ordinary Least Square (OLS) method is commonly used to estimate the unknown regression parameter. The OLS estimate in case of linear regression model is defined as follows, $$\hat{\beta} = (X^T X)^{-1} X^T y.$$ {#eq-1}

One of the key assumption of the widely used multiple linear regression model is that the predictors need to be independent of each other. Violating the assumption of independence results in an issue known as multicollinearity. Multicollinearity, originally identified by @frisch1934statistical, is the state in which two or more independent variables show a strong correlation with one another. It causes standard error of the OLS estimator to increase resulting in wide confidence intervals and less reliable results. Identifying and interpreting significant predictors also becomes difficult [@saleh2019theory]. Increasing the sample size, eliminating the highly correlated variables, Principal component analysis are some of the ways to avoid multicollinearity but with the risk of losing either valuable information or interpretability. As the field of study progressed, researchers developed a number of methods that outperforms OLS in presence of multicollinearity in data by introducing bias to the estimator to gain smaller variance. Ridge regression [@hoerl1970ridge], Lasso [@tibshirani1996regression], Stein estimator [@stein1956inadmissibility], Modified ridge regression, Liu estimator [@liu1993new], Kibria-Lukman estimator [@kibria2020new] are some of them. Among these, ridge regression has become one of the most popular methods for dealing with multicollinearity, providing a robust substitute for OLS. However, Ridge regression approach requires precise ridge parameter estimation, because the number of parameters, sample size, degree of correlation, and standard error vary greatly in real-world data. As a results numerous ridge parameters estimators has been suggested researchers till date. @mermi2024new in their recent paper presented and compared a total of 366 different ridge parameters estimators. Typically, the performance of various ridge settings are compared to OLS based on their mean square error (MSE). Nonetheless, certain unknown characteristics in the model determines when Ridge Regression estimators outperform Least Squares estimators in terms of MSE [@crivelli1995confidence]. However, that does not tell us how well the corresponding ridge regression model performs in terms of finding out significance of the predictors which is one of the key points in regression analysis.There are two ways by which statistical significance of the independent variables is determined. One is the method of hypothesis testing and the other one is the method of constructing Confidence Interval (CI).

On the basis of power of the hypothesis test, a number of comparative studies have been done for ridge regression with various tuning settings [@halawa2000tests, @cule2011significance, @gokpinar2016study @perez2020some]. Nevertheless, when evaluating regression parameters, confidence intervals are preferable to hypothesis testing because they offer a range of plausible values that represent the accuracy, direction and magnitude of the estimate. They provide a more precise grasp of practical significance based on sample size while avoiding the drawbacks of p-values [@nickerson2000null] . In practical fields such as, medical studies investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant [@gardner1986confidence]. Therefore, CI's can be considered as more informative and transparent for making decisions.

The inference for ridge regression based on confidence interval has not received much attention in the statistical literature. The earliest paper found was written by @obenchain1977classical. According to his results, ridge regression is a useful technique for handling multicollinear data without sacrificing the validity of statistical inference based on F and t tests, despite its bias [@obenchain1977classical]. @vinod1987confidence studied four alternative methods to calculate confidence intervals ridge regression estimation. @crivelli1995confidence took into consideration two confidence intervals for the ridge regression parameters.The first is based on the standard normal distribution, while the second is based on the Edgeworth expansion of the standard normal distribution statistic, where the bootstrap concept is used to determine the percentile. A similar comparative study is done by @chaubey2018confidence where they compared confidence intervals constructed by Normal theory method, Percentile method, Studentized-t method, and Bias corrected accelerated method using Bootstarp and Jackknife methodology. @revan2023bootstrap in their most recent published work investigated confidence intervals constructed by standard normal approximation, student-t approximation and bootstrap method for different regularization parameters. Their finding indicate that both mean interval length and the coverage probability is affected by the choice of the shrinkage parameter.

This research examines the Confidence Intervals of multiple linear ridge regression settings for a number of shrinkage parameter under identical simulation conditions. The comparison is conducted based on the width of the confidence interval and coverage probability. The research might be useful to evaluate if shrinkage has produced more stable and dependable estimates than ordinary least squares (OLS) regression by evaluating the bias and variance reduction in Ridge Regression. We can also determine which tuning parameter gives a high coverage probability with a comparatively narrow indicating higher precision.

--------\[ aim needs to be updated based on discussion\]

The rest of the paper is organized as follows,

------ \[write about different sections.\]

# Statistical Methodology

## Confidence Interval for OLS Estimators

The OLS estimator $\hat{\beta}$ is an unbiased estimator of the ${\beta}$. Under the assumptions od the classical linear regression model the OLS estimator $\hat{\beta}$ is calculated by minimizing the sum of square of the residuals. To measure the variability of the estimate across samples the standard error is calculated as follows, $$ \text{SE}(\hat{\beta}) = \sqrt{\frac{\sigma^2}{\sum (x_i - \bar{x})^2}},
$$ {#eq-2} where, $\sigma^2$ is the variance of the error term. Hence the confidence interval for $\hat{\beta}$ is given as, $$ \hat{\beta} \pm t_{\alpha/2, n-k} \cdot \text{SE}(\hat{\beta})
$$ {#eq-3} where, $t_{\alpha/2, n-k}$ is the critical value for t- distribution with $n - k$ degrees of freedom for $n$ number of samples and $k$ predictors.

## Confidence Interval for Ridge Regression Estimators

The canonical form of model @eq-1 is written as, $$ y = Z\alpha + \epsilon, $$ {#eq-4} where, $Z = X Q$, $\alpha = Q^T \beta$ assuming $Q$ is a $p \times p$ orthogonal matrix, and $Z^T Z = \Lambda$ is a $p \times p$ diagonal matrix with eigen values ($\lambda_i$) as diagonal elements. The ridge regression estimator with k as the shrinkage parameter proposed by @hoerl1970ridge for $\alpha$ is written as, $$ \hat{\alpha} (k) = (Z^TZ + kI_p)^{-1} Z^Ty = W\hat{\alpha} $$ {#eq-5} where, $W = [I_p + k C^{-1}]^{-1}$ is the ridge regression estimator and $I_p$ is a identity matrix of order p. When there is multicollinearity among the regressors, $X^TX$ becomes singular or nearly singular, which means it cannot be inverted. Introducing a small positive number $kI_p$ to the diagonal elements making it always invertible thus it will always have a unique solution.

## Ridge regression parameters

CI for ridge,

different k's

# Simulation Study

# Application

# Discussion and Conclusion

# Bibliography
