---
title: "Comparison of Confidence Interval in the OLS and Ridge Linear Regression Model: A Comparative study via Simulation"
format: #docx
    tandf-pdf:
      keep-tex: true  
    tandf-html: default
author:
  - name: Sultana Mubarika Rahman Chowdhury
    affiliations:
      - ref: biostat
    orcid: 
    email: schow034@fiu.edu
  - name: B.M. Golam Kibria
    affiliations:
      - ref: stat
  - name: Zoran Bursac
    affiliations:
      - ref: biostat
affiliations:
  - id: biostat
    name: Florida International University
    department: Biosatistics Department
    address: 11200 SW 8th St
    city: Miami, Fl
    country: USA
    postal-code: 33199
  - id: stat
    name: Florida International University
    department: Department of Mathematics and Statistics
    address: 11200 SW 8th St 
    city: Miami, Fl
    country: USA
    postal-code: 33199
abstract: |
  Write down the abstract here
keywords: 
  - Multiple Linear Regression
  - Ridge Regression
  - Multicolinearity
bibliography: bibliography.bib  
---

# Introduction

Multiple linear regression maps the relationship between two or more predictors and dependent variable to a linear equation. The aim is to predict the response variable using the independent variables. If X is a n×p full rank matrix of predictors and Y is a n × 1 vector of response variables the multiple linear regression can be explained as,

$$Y = X \times \beta + \epsilon, $$ where, $\beta$ is an p × 1 unknown regression paramaters and $\epsilon$ is the n × 1 vector of error with mean zero and equal variance.Ordinary Least Square (OLS) method is commonly used to estimate the unknown regression parameter. The OLS estimate in case of linear regression model is defined as follows,

$$\hat{\beta} = (X^T X)^{-1} X^T y.$$

One of the key assumption of the widely used multiple linear regression model is that the predictors need to be independent of each other. Violating the assumption of independence results in an issue known as multicollinearity. Multicollinearity, originally identified by @frisch1934statistical, is the state in which two or more independent variables show a strong correlation with one another. It causes standard error of the OLS estimator to increase resulting in wide confidence intervals and less reliable results. Identifying and interpreting significant predictors also becomes difficult [@saleh2019theory]. Increasing the sample size, eliminating the highly correlated variables, Principal component analysis are some of the ways to avoid multicollinearity but with the risk of losing either valuable information or interpretability. 

As the field of study progressed, researchers developed a number of methods that outperforms OLS in presence of multicollinearity in data by introducing bias to the estimator to gain smaller variance. Ridge regression [@hoerl1970ridge], Lasso [@tibshirani1996regression], Stein estimator [@stein1956inadmissibility], Modified ridge regression, Liu estimator [@liu1993new], Kibria-Lukman estimator [@kibria2020new] are some of them.  Among these, ridge regression has become one of the most popular methods for dealing with multicollinearity, providing a robust substitute for OLS. However, Ridge regression approach requires precise ridge parameter estimation, because the number of parameters, sample size, degree of correlation, and standard error vary greatly in real-world data. As a results numerous ridge parameters estimators has been suggested researchers till date. @mermi2024new in their recent paper presented and compared a total of 366 different ridge parameters estimators. 

Typically, the performance of various ridge settings are compared to OLS based on their mean square error (MSE). Nonetheless, certain unknown characteristics in the model determines when Ridge Regression estimators outperform Least Squares estimators in terms of MSE [@crivelli1995confidence]. However, that does not tell us how well the corresponding ridge regression model performs in terms of finding out significance of the predictors which is one of the key points in regression analysis.

There are two ways by which statistical significance of the independent variables is determined. One is the method of hypothesis testing and the other one is the method of constructing Confidence Interval (CI). On the basis of power of the hypothesis test, a number of comparative studies have been done for ridge regression with various tuning settings [@halawa2000tests, @cule2011significance, @gokpinar2016study @perez2020some]. Nevertheless, when evaluating regression parameters, confidence intervals are preferable to hypothesis testing because they offer a range of plausible values that represent the accuracy, direction and magnitude of the estimate. They provide a more precise grasp of practical significance based on sample size while avoiding the drawbacks of p-values [@nickerson2000null] . In practical fields such as, medical studies investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant [@gardner1986confidence]. Therefore, CI's can be considered as more informative and transparent for making decisions.

This research examines the Confidence Intervals of multiple linear ridge regression settings for a number of shrinkage parameter under identical simulation conditions. The comparison is conducted based on the width of the confidence interval and coverage probability. The research might be useful to evaluate if shrinkage has produced more stable and dependable estimates than ordinary least squares (OLS) regression by evaluating the bias and variance reduction in Ridge Regression. We can also determine which tuning parameter gives a high coverage probability with a comparatively narrow indicating higher precision.

The rest of the paper is organized as follows,  




























# Statistical Methodology

## Ridge Regression Estimators

## Confidence Interval

## CI for Ridge regression

# Simulation Study

# Application

# Discussion and Conclusion

# Bibliography
